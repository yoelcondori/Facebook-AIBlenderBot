{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UahVKS1KXfID"
      },
      "source": [
        " BlenderBot de Facebook \n",
        "\n",
        "> Notebook adaptado de [este cuaderno](https://colab.research.google.com/drive/1NX-GFLeMpQm9fZXlclPs9sB30Z-IA6T7) de [Quantum Stat.](http://www.quantumstat.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRZHRPNDp_R8"
      },
      "source": [
        "Ejecuta el siguiente comando para comprobar que tienes la aceleraci贸n por hardware con la GPU disponible. En caso de estar habilitada, podr谩s activarla en el men煤 'Entorno de ejecuci贸n' > 'Cambiar tipo de entorno de ejecuci贸n' > 'Acelerador por Hardware'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3AfX0Do8BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d343abcf-b42c-4510-ff76-8d68953a8947"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 12 20:33:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPmSE5bDhaWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7557f917-67f1-46d7-ea64-48dbc452c8f0"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/ParlAI.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ParlAI'...\n",
            "remote: Enumerating objects: 46800, done.\u001b[K\n",
            "remote: Counting objects: 100% (717/717), done.\u001b[K\n",
            "remote: Compressing objects: 100% (366/366), done.\u001b[K\n",
            "remote: Total 46800 (delta 426), reused 593 (delta 327), pack-reused 46083\u001b[K\n",
            "Receiving objects: 100% (46800/46800), 140.95 MiB | 17.08 MiB/s, done.\n",
            "Resolving deltas: 100% (33222/33222), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFMx_kohwC7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d114457e-ab08-42e8-f761-6b96919b8ae7"
      },
      "source": [
        "cd ParlAI/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ParlAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1YoG7VwDbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c68ed66-e514-482b-d262-8221f86932ff"
      },
      "source": [
        "!python setup.py develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, Python >=3.8 is required for ParlAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flvyweIpwE4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fff9aed-a733-4f97-e3a0-bca6b7494290"
      },
      "source": [
        "!pip install 'git+https://github.com/rsennrich/subword-nmt.git#egg=subword-nmt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting subword-nmt\n",
            "  Cloning https://github.com/rsennrich/subword-nmt.git to /tmp/pip-install-kf0tqc3x/subword-nmt_c56caa9579024c9691b052130991c2bf\n",
            "  Running command git clone -q https://github.com/rsennrich/subword-nmt.git /tmp/pip-install-kf0tqc3x/subword-nmt_c56caa9579024c9691b052130991c2bf\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from subword-nmt) (4.64.1)\n",
            "Building wheels for collected packages: subword-nmt\n",
            "  Building wheel for subword-nmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subword-nmt: filename=subword_nmt-0.3.8-py3-none-any.whl size=130557 sha256=800257bef66d19516c2021c2f138c1412e41a595e88b65a333501d010df937e2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i0sruocl/wheels/33/cb/e7/2c8361fa917e67109ef9c53a0c92c9477f33c0544d6410e1bb\n",
            "Successfully built subword-nmt\n",
            "Installing collected packages: mock, subword-nmt\n",
            "Successfully installed mock-4.0.3 subword-nmt-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9NEQhOiX0Sg"
      },
      "source": [
        "# Facebook BlenderBot **2.7B**\n",
        "\n",
        "Instrucciones de uso del modelo BlenderBot: \n",
        "> Ejecuta la celda a continuaci贸n para ejecutar el script ***interactive.py*** y as铆 poder arrancar al bot. En caso de querer ejecutar la versi贸n que s贸lo admite y ofrece respuestas correctas (sin lenguaje agresivo, ofensivo, etc) deber谩s de ejecutar el modelo ***safe_interactive.py.***\n",
        "\n",
        "> De los tres modelos facilitados por [FacebookAI](https://parl.ai/projects/recipes/) *(Small: **90M**, Medium: **2.7B**, Big: **9.4B**)*  el siguiente comando ejecutar谩 el modelo de tama帽o medio: ***blender_3B***. En funci贸n del hardware disponible podr铆as probar el modelo de mayor tama帽o en tu propio sistema haciendo uso de los mismo comandos.\n",
        "\n",
        "> Durante el di谩logo con BlenderBot, podr谩s ejecutar el comando **[DONE]** para finalizar la conversaci贸n y generar un nuevo chatbot con nueva personalidad con el que poder hablar. Ver谩s al final de cada conversaci贸n finalizada cu谩l era la personalidad interpretada por tu chatbot."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install parlai\n"
      ],
      "metadata": {
        "id": "n7OPmvSaVxg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59be87dc-9500-49af-f28a-1ece3f9f31cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting parlai\n",
            "  Downloading parlai-1.7.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     || 1.9 MB 34.5 MB/s \n",
            "\u001b[?25hCollecting fsspec~=2022.2.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     || 134 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from parlai) (23.2.1)\n",
            "Requirement already satisfied: torchtext>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (0.13.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (2.23.0)\n",
            "Collecting sh\n",
            "  Downloading sh-1.14.3.tar.gz (62 kB)\n",
            "\u001b[K     || 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting markdown<=3.3.2\n",
            "  Downloading Markdown-3.3.2-py3-none-any.whl (95 kB)\n",
            "\u001b[K     || 95 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from parlai) (3.6.4)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     || 55 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting py-gfm\n",
            "  Downloading py_gfm-1.0.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (1.12.1+cu113)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from parlai) (1.18.1)\n",
            "Collecting omegaconf>=2.1.1\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[K     || 79 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from parlai) (7.1.2)\n",
            "Collecting py-rouge\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     || 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting pytest-regressions\n",
            "  Downloading pytest_regressions-2.4.1-py3-none-any.whl (23 kB)\n",
            "Collecting tokenizers>=0.8.0\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     || 7.6 MB 53.8 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     || 125 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     || 140 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from parlai) (1.3.5)\n",
            "Collecting contractions~=0.1.72\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from parlai) (1.0.2)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting datasets<2.2.2,>=1.4.1\n",
            "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
            "\u001b[K     || 342 kB 72.4 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1.0\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     || 151 kB 68.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from parlai) (3.7)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     || 46 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting flake8-bugbear\n",
            "  Downloading flake8_bugbear-22.9.23-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from parlai) (2022.6.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from parlai) (1.7.3)\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     || 235 kB 74.6 MB/s \n",
            "\u001b[?25hCollecting fairscale~=0.4.1\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     || 248 kB 66.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from parlai) (0.3.8)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from parlai) (4.8.0)\n",
            "Collecting iopath~=0.1.8\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     || 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.3\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting gitdb2\n",
            "  Downloading gitdb2-4.0.2-py3-none-any.whl (1.1 kB)\n",
            "Collecting attrs~=20.2.0\n",
            "  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     || 48 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from parlai) (6.0)\n",
            "Collecting docformatter\n",
            "  Downloading docformatter-1.5.0-py3-none-any.whl (14 kB)\n",
            "Collecting flake8\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     || 61 kB 591 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from parlai) (5.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from parlai) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from parlai) (4.1.1)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     || 2.8 MB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (2.0.1)\n",
            "Collecting parlai\n",
            "  Downloading parlai-1.7.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     || 1.9 MB 62.6 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Using cached ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "Collecting parlai\n",
            "  Downloading parlai-1.6.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     || 1.7 MB 56.7 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.24.89-py3-none-any.whl (132 kB)\n",
            "\u001b[K     || 132 kB 70.3 MB/s \n",
            "\u001b[?25hCollecting hydra-core~=1.1.0\n",
            "  Downloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     || 147 kB 81.3 MB/s \n",
            "\u001b[?25hCollecting omegaconf~=2.1.1\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[K     || 74 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting datasets>=1.4.1\n",
            "  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n",
            "\u001b[K     || 432 kB 81.0 MB/s \n",
            "\u001b[?25hCollecting numpy<=1.21\n",
            "  Downloading numpy-1.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     || 15.7 MB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from parlai) (2.8.0)\n",
            "Collecting botocore\n",
            "  Downloading botocore-1.27.89-py3-none-any.whl (9.2 MB)\n",
            "\u001b[K     || 9.2 MB 55.3 MB/s \n",
            "\u001b[?25hCollecting websocket-server\n",
            "  Downloading websocket_server-0.6.4-py3-none-any.whl (7.5 kB)\n",
            "Collecting tomli<2.0.0\n",
            "  Downloading tomli-1.2.3-py3-none-any.whl (12 kB)\n",
            "Collecting myst-parser~=0.12.2\n",
            "  Downloading myst_parser-0.12.10-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from parlai) (7.9.0)\n",
            "Collecting GitPython\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     || 182 kB 71.6 MB/s \n",
            "\u001b[?25hCollecting requests-mock\n",
            "  Downloading requests_mock-1.10.0-py2.py3-none-any.whl (28 kB)\n",
            "Collecting Sphinx~=2.2.0\n",
            "  Downloading Sphinx-2.2.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     || 2.7 MB 63.1 MB/s \n",
            "\u001b[?25hCollecting tqdm~=4.62.1\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     || 76 kB 229 kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     || 216 kB 73.8 MB/s \n",
            "\u001b[?25hCollecting docutils<0.16,>=0.14\n",
            "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
            "\u001b[K     || 547 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting sphinx-autodoc-typehints~=1.10.3\n",
            "  Downloading sphinx_autodoc_typehints-1.10.3-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from parlai) (2.11.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     || 163 kB 77.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (2022.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (0.3.5.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     || 212 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     || 115 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.4.1->parlai) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.4.1->parlai) (3.8.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     || 112 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting importlib-resources<5.3\n",
            "  Downloading importlib_resources-5.2.3-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->parlai) (3.8.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting markdown-it-py~=0.5.4\n",
            "  Downloading markdown_it_py-0.5.8-py3-none-any.whl (110 kB)\n",
            "\u001b[K     || 110 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.4.1->parlai) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (3.0.4)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     || 62 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (1.1.5)\n",
            "Collecting sphinxcontrib-htmlhelp\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     || 100 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (57.4.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (0.7.12)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     || 121 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (1.4.1)\n",
            "Collecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     || 84 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.10.3)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     || 90 kB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.2.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->Sphinx~=2.2.0->parlai) (2022.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->parlai) (2.0.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     || 79 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore->parlai) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore->parlai) (1.15.0)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     || 86 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting docformatter\n",
            "  Downloading docformatter-1.4.tar.gz (208 kB)\n",
            "\u001b[K     || 208 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting untokenize\n",
            "  Downloading untokenize-0.1.1.tar.gz (3.1 kB)\n",
            "Collecting pyflakes<2.6.0,>=2.5.0\n",
            "  Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     || 66 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.8.0,>=0.7.0\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pycodestyle<2.10.0,>=2.9.0\n",
            "  Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     || 41 kB 486 kB/s \n",
            "\u001b[?25hCollecting gitdb>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     || 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->parlai) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->parlai) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->parlai) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->parlai) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->parlai) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->parlai) (0.2.8)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->parlai) (1.31.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->parlai) (1.56.4)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->parlai) (3.17.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->parlai) (0.4.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (0.2.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     || 1.6 MB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->parlai) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->parlai) (0.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->parlai) (7.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->parlai) (0.7.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (8.14.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (1.4.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (0.7.1)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     || 298 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting pytest-datadir>=1.2.0\n",
            "  Downloading pytest_datadir-1.3.1-py2.py3-none-any.whl (5.9 kB)\n",
            "Collecting pytest\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[K     || 297 kB 75.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
            "\u001b[K     || 297 kB 78.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     || 297 kB 78.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[K     || 296 kB 71.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-7.0.0-py3-none-any.whl (296 kB)\n",
            "\u001b[K     || 296 kB 77.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     || 280 kB 82.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (0.10.2)\n",
            "  Downloading pytest-6.2.4-py3-none-any.whl (280 kB)\n",
            "\u001b[K     || 280 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pytest\n",
            "  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     || 280 kB 69.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.2-py3-none-any.whl (280 kB)\n",
            "\u001b[K     || 280 kB 79.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     || 279 kB 55.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.0-py3-none-any.whl (279 kB)\n",
            "\u001b[K     || 279 kB 80.3 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-regressions to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-regressions\n",
            "  Downloading pytest_regressions-2.4.0-py3-none-any.whl (23 kB)\n",
            "  Downloading pytest_regressions-2.3.1-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->parlai) (3.1.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword-nmt->parlai) (4.0.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.49.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (3.2.1)\n",
            "Building wheels for collected packages: fairscale, antlr4-python3-runtime, iopath, docformatter, emoji, sh, untokenize\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=72b8ed160751d4af087ba6a2b3d5b8c365bacc9780322874eb7aa893be6fa26a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=f21308b298ef8b1201a4f877583b15cb4307fed3b2ea78af20ce78e683a545a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31549 sha256=6edaedc3532a4780bfbfcfe2ca6470ac1f6da198eb68604566f00ddd8828743a\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/cc/ed/ca4e88beef656b01c84b9185196513ef2faf74a5a379b043a7\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docformatter: filename=docformatter-1.4-py3-none-any.whl size=12405 sha256=cbba3acf8be46de70f4acf73692ca167d96d8c11218d751ddde6c050ca49353c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/62/81/a91b835e57388111fbe5f8c0cc99c5be6e257ba1fda172dd19\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=31e5a10cc4efe15b9725c58f75581c2c6572bb371d0417074ace371255f13616\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "  Building wheel for sh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sh: filename=sh-1.14.3-py2.py3-none-any.whl size=39656 sha256=733ee129719f8765c1fc3c3323fe9c684fd2b32f3ff1fb70b8fad5e74cbf700d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/d2/e1/3290bef6382cfc1ec5e659353e5f403fc1eb385ea5f615519a\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for untokenize: filename=untokenize-0.1.1-py3-none-any.whl size=2888 sha256=551db0e5bc5222407de7a0e857a86f8596b6a55b622ba2fb1754f04113b074e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/d4/22/3b662355e9a2faa5fe462c17b6fae2e9757066c36cd72c4497\n",
            "Successfully built fairscale antlr4-python3-runtime iopath docformatter emoji sh untokenize\n",
            "Installing collected packages: urllib3, requests, jmespath, attrs, tqdm, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, smmap, pyflakes, pycodestyle, numpy, mccabe, importlib-metadata, docutils, botocore, antlr4-python3-runtime, xxhash, untokenize, Sphinx, s3transfer, responses, pytest-datadir, portalocker, omegaconf, multiprocess, markdown-it-py, markdown, jedi, importlib-resources, humanfriendly, huggingface-hub, gitdb, flake8, websocket-server, websocket-client, Unidecode, tomli, tokenizers, tensorboardX, sphinx-rtd-theme, sphinx-autodoc-typehints, sh, requests-mock, pytest-regressions, py-rouge, py-gfm, myst-parser, jsonlines, iopath, hydra-core, GitPython, gitdb2, flake8-bugbear, fairscale, emoji, docformatter, datasets, coloredlogs, boto3, parlai\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.1.0\n",
            "    Uninstalling attrs-22.1.0:\n",
            "      Successfully uninstalled attrs-22.1.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.0.0\n",
            "    Uninstalling importlib-metadata-5.0.0:\n",
            "      Successfully uninstalled importlib-metadata-5.0.0\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: Sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.4.1\n",
            "    Uninstalling Markdown-3.4.1:\n",
            "      Successfully uninstalled Markdown-3.4.1\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.9.0\n",
            "    Uninstalling importlib-resources-5.9.0:\n",
            "      Successfully uninstalled importlib-resources-5.9.0\n",
            "  Attempting uninstall: tomli\n",
            "    Found existing installation: tomli 2.0.1\n",
            "    Uninstalling tomli-2.0.1:\n",
            "      Successfully uninstalled tomli-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 Sphinx-2.2.2 Unidecode-1.3.6 antlr4-python3-runtime-4.8 attrs-20.2.0 boto3-1.24.89 botocore-1.27.89 coloredlogs-15.0.1 datasets-2.5.2 docformatter-1.4 docutils-0.15.2 emoji-2.1.0 fairscale-0.4.6 flake8-5.0.4 flake8-bugbear-22.9.23 gitdb-4.0.9 gitdb2-4.0.2 huggingface-hub-0.10.1 humanfriendly-10.0 hydra-core-1.1.2 importlib-metadata-4.2.0 importlib-resources-5.2.3 iopath-0.1.10 jedi-0.18.1 jmespath-1.0.1 jsonlines-3.1.0 markdown-3.3.2 markdown-it-py-0.5.8 mccabe-0.7.0 multiprocess-0.70.13 myst-parser-0.12.10 numpy-1.21.0 omegaconf-2.1.2 parlai-1.6.0 portalocker-2.5.1 py-gfm-1.0.2 py-rouge-1.1 pycodestyle-2.9.1 pyflakes-2.5.0 pytest-datadir-1.3.1 pytest-regressions-2.3.1 requests-2.28.1 requests-mock-1.10.0 responses-0.18.0 s3transfer-0.6.0 sh-1.14.3 smmap-5.0.0 sphinx-autodoc-typehints-1.10.3 sphinx-rtd-theme-1.0.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 tensorboardX-2.5.1 tokenizers-0.13.1 tomli-1.2.3 tqdm-4.62.3 untokenize-0.1.1 urllib3-1.26.12 websocket-client-1.4.1 websocket-server-0.6.4 xxhash-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pydevd_plugins",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "HrA5JQ4lX256"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jImNbsQSX6b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdC8nvnLY_Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0544e4a-21de-458a-b365-1af8c9564e67"
      },
      "source": [
        "!python /content/ParlAI/parlai/scripts/interactive.py -t blended_skill_talk -mf zoo:blender/blender_3B/model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...       B l e n d e r B o t ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&/\n",
            "20:34:42 | building data: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/BST3B.tgz\n",
            "20:34:42 | Downloading http://parl.ai/downloads/_models/blender/BST3B.tgz to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/BST3B.tgz\n",
            "Downloading BST3B.tgz: 100% 4.95G/4.95G [09:18<00:00, 8.87MB/s]\n",
            "20:45:34 | \u001b[33mOverriding opt[\"task\"] to blended_skill_talk (previously: internal:blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues)\u001b[0m\n",
            "20:45:34 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model (previously: /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/de6/model)\u001b[0m\n",
            "20:45:34 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "20:45:34 | Using CUDA\n",
            "20:45:34 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model.dict\n",
            "20:45:34 | num words = 8008\n",
            "20:45:34 | TransformerGenerator: full interactive mode on.\n",
            "20:46:24 | Total parameters: 2,696,268,800 (2,695,613,440 trainable)\n",
            "20:46:24 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model\n",
            "20:46:47 | Opt:\n",
            "20:46:47 |     activation: gelu\n",
            "20:46:47 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "20:46:47 |     adam_eps: 1e-08\n",
            "20:46:47 |     add_p1_after_newln: False\n",
            "20:46:47 |     aggregate_micro: False\n",
            "20:46:47 |     allow_missing_init_opts: False\n",
            "20:46:47 |     attention_dropout: 0.0\n",
            "20:46:47 |     batchsize: 128\n",
            "20:46:47 |     beam_block_full_context: False\n",
            "20:46:47 |     beam_block_list_filename: None\n",
            "20:46:47 |     beam_block_ngram: 3\n",
            "20:46:47 |     beam_context_block_ngram: 3\n",
            "20:46:47 |     beam_delay: 30\n",
            "20:46:47 |     beam_length_penalty: 0.65\n",
            "20:46:47 |     beam_min_length: 20\n",
            "20:46:47 |     beam_size: 10\n",
            "20:46:47 |     betas: '[0.9, 0.999]'\n",
            "20:46:47 |     bpe_add_prefix_space: True\n",
            "20:46:47 |     bpe_debug: False\n",
            "20:46:47 |     bpe_dropout: None\n",
            "20:46:47 |     bpe_merge: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model.dict-merges.txt\n",
            "20:46:47 |     bpe_vocab: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model.dict-vocab.json\n",
            "20:46:47 |     checkpoint_activations: False\n",
            "20:46:47 |     compute_tokenized_bleu: False\n",
            "20:46:47 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "20:46:47 |     datatype: train\n",
            "20:46:47 |     delimiter: '  '\n",
            "20:46:47 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "20:46:47 |     dict_endtoken: __end__\n",
            "20:46:47 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model.dict\n",
            "20:46:47 |     dict_include_test: False\n",
            "20:46:47 |     dict_include_valid: False\n",
            "20:46:47 |     dict_initpath: None\n",
            "20:46:47 |     dict_language: english\n",
            "20:46:47 |     dict_loaded: True\n",
            "20:46:47 |     dict_lower: False\n",
            "20:46:47 |     dict_max_ngram_size: -1\n",
            "20:46:47 |     dict_maxexs: -1\n",
            "20:46:47 |     dict_maxtokens: -1\n",
            "20:46:47 |     dict_minfreq: 0\n",
            "20:46:47 |     dict_nulltoken: __null__\n",
            "20:46:47 |     dict_starttoken: __start__\n",
            "20:46:47 |     dict_textfields: text,labels\n",
            "20:46:47 |     dict_tokenizer: bytelevelbpe\n",
            "20:46:47 |     dict_unktoken: __unk__\n",
            "20:46:47 |     display_add_fields: \n",
            "20:46:47 |     display_examples: False\n",
            "20:46:47 |     display_partner_persona: True\n",
            "20:46:47 |     display_prettify: False\n",
            "20:46:47 |     download_path: None\n",
            "20:46:47 |     dropout: 0.1\n",
            "20:46:47 |     dynamic_batching: None\n",
            "20:46:47 |     embedding_projection: random\n",
            "20:46:47 |     embedding_size: 2560\n",
            "20:46:47 |     embedding_type: random\n",
            "20:46:47 |     embeddings_scale: True\n",
            "20:46:47 |     eval_batchsize: None\n",
            "20:46:47 |     evaltask: None\n",
            "20:46:47 |     ffn_size: 10240\n",
            "20:46:47 |     force_fp16_tokens: True\n",
            "20:46:47 |     fp16: True\n",
            "20:46:47 |     fp16_impl: mem_efficient\n",
            "20:46:47 |     gpu: -1\n",
            "20:46:47 |     gradient_clip: 0.1\n",
            "20:46:47 |     hide_labels: False\n",
            "20:46:47 |     history_add_global_end_token: end\n",
            "20:46:47 |     history_reversed: False\n",
            "20:46:47 |     history_size: -1\n",
            "20:46:47 |     image_cropsize: 224\n",
            "20:46:47 |     image_mode: raw\n",
            "20:46:47 |     image_size: 256\n",
            "20:46:47 |     include_checked_sentence: True\n",
            "20:46:47 |     include_initial_utterances: False\n",
            "20:46:47 |     include_knowledge: True\n",
            "20:46:47 |     include_knowledge_separator: False\n",
            "20:46:47 |     include_personas: True\n",
            "20:46:47 |     inference: beam\n",
            "20:46:47 |     init_model: /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model\n",
            "20:46:47 |     init_opt: None\n",
            "20:46:47 |     interactive_mode: True\n",
            "20:46:47 |     interactive_task: True\n",
            "20:46:47 |     invsqrt_lr_decay_gamma: -1\n",
            "20:46:47 |     is_debug: False\n",
            "20:46:47 |     label_truncate: 128\n",
            "20:46:47 |     label_type: response\n",
            "20:46:47 |     learn_positional_embeddings: False\n",
            "20:46:47 |     learningrate: 7e-06\n",
            "20:46:47 |     local_human_candidates_file: None\n",
            "20:46:47 |     log_every_n_secs: 10.0\n",
            "20:46:47 |     log_keep_fields: all\n",
            "20:46:47 |     loglevel: info\n",
            "20:46:47 |     lr_scheduler: reduceonplateau\n",
            "20:46:47 |     lr_scheduler_decay: 0.5\n",
            "20:46:47 |     lr_scheduler_patience: 3\n",
            "20:46:47 |     max_lr_steps: -1\n",
            "20:46:47 |     max_train_time: 27647.999999999996\n",
            "20:46:47 |     metrics: default\n",
            "20:46:47 |     model: transformer/generator\n",
            "20:46:47 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model\n",
            "20:46:47 |     model_parallel: True\n",
            "20:46:47 |     momentum: 0\n",
            "20:46:47 |     multitask_weights: '[1.0, 3.0, 3.0, 3.0]'\n",
            "20:46:47 |     mutators: None\n",
            "20:46:47 |     n_decoder_layers: 24\n",
            "20:46:47 |     n_encoder_layers: 2\n",
            "20:46:47 |     n_heads: 32\n",
            "20:46:47 |     n_layers: 2\n",
            "20:46:47 |     n_positions: 128\n",
            "20:46:47 |     n_segments: 0\n",
            "20:46:47 |     nesterov: True\n",
            "20:46:47 |     no_cuda: False\n",
            "20:46:47 |     num_epochs: -1\n",
            "20:46:47 |     num_topics: 5\n",
            "20:46:47 |     numthreads: 1\n",
            "20:46:47 |     nus: [0.7]\n",
            "20:46:47 |     optimizer: mem_eff_adam\n",
            "20:46:47 |     outfile: \n",
            "20:46:47 |     output_scaling: 1.0\n",
            "20:46:47 |     override: \"{'task': 'blended_skill_talk', 'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/blender/blender_3B/model'}\"\n",
            "20:46:47 |     parlai_home: /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI\n",
            "20:46:47 |     person_tokens: False\n",
            "20:46:47 |     rank_candidates: False\n",
            "20:46:47 |     relu_dropout: 0.0\n",
            "20:46:47 |     remove_political_convos: False\n",
            "20:46:47 |     safe_personas_only: True\n",
            "20:46:47 |     save_after_valid: True\n",
            "20:46:47 |     save_every_n_secs: -1\n",
            "20:46:47 |     save_format: conversations\n",
            "20:46:47 |     share_word_embeddings: True\n",
            "20:46:47 |     short_final_eval: False\n",
            "20:46:47 |     show_advanced_args: False\n",
            "20:46:47 |     single_turn: False\n",
            "20:46:47 |     skip_generation: False\n",
            "20:46:47 |     special_tok_lst: None\n",
            "20:46:47 |     split_lines: False\n",
            "20:46:47 |     starttime: Mar31_06-04\n",
            "20:46:47 |     task: blended_skill_talk\n",
            "20:46:47 |     temperature: 1.0\n",
            "20:46:47 |     tensorboard_log: False\n",
            "20:46:47 |     text_truncate: 128\n",
            "20:46:47 |     topk: 10\n",
            "20:46:47 |     topp: 0.9\n",
            "20:46:47 |     train_experiencer_only: False\n",
            "20:46:47 |     truncate: 128\n",
            "20:46:47 |     update_freq: 2\n",
            "20:46:47 |     use_reply: label\n",
            "20:46:47 |     validation_cutoff: 1.0\n",
            "20:46:47 |     validation_every_n_epochs: 0.25\n",
            "20:46:47 |     validation_every_n_secs: -1\n",
            "20:46:47 |     validation_max_exs: -1\n",
            "20:46:47 |     validation_metric: ppl\n",
            "20:46:47 |     validation_metric_mode: min\n",
            "20:46:47 |     validation_patience: 10\n",
            "20:46:47 |     validation_share_agent: False\n",
            "20:46:47 |     variant: prelayernorm\n",
            "20:46:47 |     verbose: False\n",
            "20:46:47 |     warmup_rate: 0.0001\n",
            "20:46:47 |     warmup_updates: 100\n",
            "20:46:47 |     weight_decay: None\n",
            "20:46:47 | Current internal commit: 2e2a2b2ecd9baee14064c97062115bf900f7281b\n",
            "20:46:47 | Current fb commit: 2e2a2b2ecd9baee14064c97062115bf900f7281b\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "20:46:47 | creating task(s): blended_skill_talk\n",
            "[ loading personas.. ]\n",
            "\n",
            "  [NOTE: In the BST paper both partners have a persona.\n",
            "         You can choose to ignore yours, the model never sees it.\n",
            "         In the Blender paper, this was not used for humans.\n",
            "         You can also turn personas off with --include-personas False]\n",
            "\n",
            "[building data: /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk]\n",
            "20:46:47 | Downloading http://parl.ai/downloads/blended_skill_talk/blended_skill_talk.tar.gz to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/blended_skill_talk.tar.gz\n",
            "Downloading blended_skill_talk.tar.gz: 100% 38.1M/38.1M [00:04<00:00, 7.97MB/s]\n",
            "20:46:54 | Downloading http://parl.ai/downloads/blended_skill_talk/personas_list.txt to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/persona_list.txt\n",
            "Downloading persona_list.txt: 0.00B [00:02, ?B/s]\n",
            "20:46:57 | Downloading http://parl.ai/downloads/blended_skill_talk/topic_to_persona_list.txt to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/topic_to_persona_list.txt\n",
            "Downloading topic_to_persona_list.txt: 0.00B [00:01, ?B/s]\n",
            "20:46:58 | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__train__both_sides.json to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/ed_persona_topicifier__train__both_sides.json\n",
            "Downloading ed_persona_topicifier__train__both_sides.json: 0.00B [00:05, ?B/s]\n",
            "20:47:04 | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__train__experiencer_only.json to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/ed_persona_topicifier__train__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__train__experiencer_only.json: 0.00B [00:03, ?B/s]\n",
            "20:47:08 | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__valid__experiencer_only.json to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/ed_persona_topicifier__valid__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__valid__experiencer_only.json: 0.00B [00:05, ?B/s]\n",
            "20:47:13 | Downloading http://parl.ai/downloads/blended_skill_talk/ed_persona_topicifier__test__experiencer_only.json to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/ed_persona_topicifier__test__experiencer_only.json\n",
            "Downloading ed_persona_topicifier__test__experiencer_only.json: 0.00B [00:04, ?B/s]\n",
            "20:47:18 | Downloading http://parl.ai/downloads/blended_skill_talk/safe_personas_2.txt to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/safe_personas.txt\n",
            "Downloading safe_personas.txt: 0.00B [00:02, ?B/s]\n",
            "20:47:21 | Downloading http://parl.ai/downloads/blended_skill_talk/human_annotations.json to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/human_annotations.json\n",
            "Downloading human_annotations.json: 0.00B [00:02, ?B/s]\n",
            "Loading /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/train.json.\n",
            "Saving to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/train.txt\n",
            "Loading /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/valid.json.\n",
            "Saving to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/valid.txt\n",
            "Loading /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/test.json.\n",
            "Saving to /usr/local/lib/python3.7/dist-packages/data/blended_skill_talk/test.txt\n",
            "\u001b[0;34m[context]:\u001b[0;0m \u001b[1myour persona: my wife use to be a teacher.\n",
            "your persona: i love to paint pictures.\n",
            "Barbershop music\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hello\n",
            "/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py:1728: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  hyp_ids = best_idxs // voc_size\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mHi, how are you? I love winter, it is the best season. What is your favorite season?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what do you do\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mI am a barber, what about you? What do you like to do in your spare time?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m "
          ]
        }
      ]
    }
  ]
}